{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second conf call fast report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-27 09:52:28.261868\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(str(datetime.datetime.today()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_For this second report I am experimenting a new way to fastly deploy updates about my project: **Jupyter notebook**. In this stage of the project, I'd like to share more code snippets than I would in the final thesis, hence markdown support and live execution of code snippets turned out to be very useful for such purpose. Actually the two main reason why I decide to move from raw LaTeX to here are: (1) Jupyter does it faster, (2) it convert everything to LaTeX (I won't be required to make any effort to include everything from here to my LaTeX thesis document). Whether, for any reason, this method will turn out to be ineffective or time-expensive I will drop it out._ \n",
    "\n",
    "## Change log summary\n",
    "\n",
    "### Training set generator function\n",
    "Now all samples components domains are centered at the same value $c$. Such domains are determined as follows.\n",
    "\n",
    "Is defined $K = \\{k_1,...k_m\\}$ where $k_i$ is the domain radius of variable $x_i$.\n",
    "Let $x=(x_1,...,x_m) \\in X$ be a sample of the training set, then the domain of $x_i$ is $D(x_i) = [c-k_i, c+k_i]$.\n",
    "\n",
    "\n",
    "```python\n",
    "def sample_from_function(n_samples, n_features, func, domain_radius=0.5, domain_center=0.5,error_mean=0, error_std_dev=1):\n",
    "    X = []\n",
    "    y = np.zeros(n_samples)\n",
    "    w = np.ones(n_features)\n",
    "    K = np.random.uniform(domain_center - domain_radius, domain_center + domain_radius, n_features)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        x = np.zeros(n_features)\n",
    "        for j in range(n_features):\n",
    "            x[j] = np.random.uniform(-K[j] + domain_radius, K[j] + domain_radius)\n",
    "        X.append(x)\n",
    "        y[i] = func(x, w) + np.random.normal(error_mean, error_std_dev)\n",
    "\n",
    "return np.array(X), y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**. Detailed execution of `sample_from_function` with the following parameters:\n",
    "- `n_samples = 8` : amount of samples in the training set;\n",
    "- `n_features = 4` : number of features, e.g. size of each array sample;\n",
    "- `func` : classic linear function $f(\\vec{x},\\vec{w})=\\sum_{j=1}^{m} x_j w_j$ where $m$ is the size of $x$ (and $w$), so, for instance, $f((2,3),(4,1))=2*4+3*1=11$;\n",
    "- `domain_radius = 5`;\n",
    "- `domain_center = 0`;\n",
    "- `error_mean = 0`;\n",
    "- `error_std_dev = 1`: the error (noise) follows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "n_samples = 8\n",
    "n_features = 4\n",
    "func = lambda _X,_w : _X.dot(_w)\n",
    "domain_center = 0\n",
    "domain_radius = 5\n",
    "error_mean = 0\n",
    "error_std_dev = 1\n",
    "X = []\n",
    "y = np.zeros(n_samples)\n",
    "w = np.ones(n_features)\n",
    "K = np.random.uniform(domain_center - domain_radius, domain_center + domain_radius, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K = \\{k_1,...k_m\\}$ where $k_i$ is the domain radius of variable $x_i$, so the domain of $x_i$ will be $D(x_i) = [c-k_i, c+k_i]$ where $c$ is the center of the domain (`domain_center`); $c$ is the same for all $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.32375564 0.71056962 2.54217545 1.6100447 ]\n"
     ]
    }
   ],
   "source": [
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   array([6.02188807, 5.67014992, 4.96419302, 5.14972292]),\n",
      "    array([5.9642668 , 4.75911729, 3.1911183 , 6.48086813]),\n",
      "    array([3.77524004, 5.69812223, 3.93359248, 4.15090944]),\n",
      "    array([5.89155598, 4.33238006, 4.67047585, 6.58797714]),\n",
      "    array([5.78911644, 4.47464001, 6.09150392, 4.30422703]),\n",
      "    array([6.09066085, 5.44278406, 7.06243236, 5.28890016]),\n",
      "    array([5.48128054, 4.92461966, 6.29826249, 4.08255381]),\n",
      "    array([4.33756388, 5.65386185, 6.73384508, 6.09020554])]\n",
      "array([23.09383521, 20.77253317, 17.89046891, 21.22414156, 20.48504975,\n",
      "       24.51765509, 19.07431732, 23.32166701])\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_samples):\n",
    "    x = np.zeros(n_features)\n",
    "    for j in range(n_features):\n",
    "        x[j] = np.random.uniform(-K[j] + domain_radius, K[j] + domain_radius)\n",
    "    X.append(x)\n",
    "    y[i] = func(x, w) + np.random.normal(error_mean, error_std_dev)\n",
    "print(pprint.PrettyPrinter(indent=4).pformat(X))\n",
    "print(pprint.PrettyPrinter(indent=4).pformat(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New metric Real Mean Squared Error (RMSE)\n",
    "The training set is in the form of a pair (X,y) where $y_i$ is the value the target function is supposed to yield for the input $x_i$, actually, whether a noise exists in the training set, then $y_i$ differs from the real value $\\tilde{y}_i$ by a gap that, in our case, is normally distributed (with mean = `error_mean` and standard deviation = `error_std_dev`). Whereas the training set is generated with fully control over all parameters, then we know either the perturbed value $y_i$ either the real value $\\tilde{y}_i=\\mathbb{1} x_i=\\sum_{j=1}^{m}x_{ij}$, so we have an additional information to take into account in order to study the behaviour of different models.\n",
    "\n",
    "While the mean squared error\n",
    "$$MSE = \\frac{1}{N} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) (\\hat{y}_i)'$$\n",
    "regards $y_i$, the real MSE concerns $\\tilde{y}_i$, hence it's defined as\n",
    "$$RMSE = \\frac{1}{N} \\sum_{i=1}^{n} (\\tilde{y}_i - \\hat{y}_i) (\\hat{y}_i)'.$$\n",
    "\n",
    "By comparing these two metrics one can understand whether and how much the prediction model suffers from noise fitting, e.g. when the model adapts itself much more on the noise rather than on the provided target function value.\n",
    "\n",
    "Obviously, noiseless training sets lead the MSE to be equal to the RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the error over the whole training set\n",
    "After each iteration each node computes a set of metrics taking into account its local model and knowledge, so each node keeps track of the history of its weight vector, MAE (mean absolute error), RMAE (to be implemented yet), MSE, RMSE.\n",
    "\n",
    "Previously I computed the error as the mean of the MSE of each node. A way that, at first glance, could seem to be reasonable, but, actually, it is not: such MSE is not computed with one weight vector over the entire training set, indeed if someone asked for the weight vector which had produced such result, then we haven't a value to provide they with.\n",
    "That's why the correct way to compute the global metrics is to retrieve from each node $k$ its local model $\\vec{w}_{k}$, compute $\\vec{w} = \\frac{1}{K}\\sum_{k=1}^{K}\\vec{w}_k$ and then computes metrics taking into account $\\vec{w}$ along with the whole training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test description\n",
    "Since there are several parameters to set up in order to run a test, I have excluded to pass them from the command line (by running the program in a way like `$ python main.py [parameters]`), instead they're are set directly inside the script `main.py`. Leave aside for a while the system and training task setup, there are many other settings which ensure control over the test execution and outputs. Without deepening their implementation, when a new test is run, the simulator creates a new folder `/$TEST_NAME` in `/test_log` that contains:\n",
    "- a `/plot` folder with all plots images;\n",
    "- all global logs of the simulation for each topology;\n",
    "- the descriptor file that reports the detailed parameters values used for the test;\n",
    "- a serialization of the setup that can be used to run again the same simulation.\n",
    "\n",
    "**Example**.\n",
    "```\n",
    "./test_log\n",
    "└── /$TEST_NAME\n",
    "    ├── /plot\n",
    "    │   ├── iter_time.png\n",
    "    │   ├── mse_iter.png\n",
    "    │   ├── mse_time.png\n",
    "    │   ├── real-mse_iter.png\n",
    "    │   └── real-mse_time.png\n",
    "    ├── clique_global_mean_squared_error_log\n",
    "    ├── clique_global_real_mean_squared_error_log\n",
    "    ├── clique_iterations_time_log\n",
    "    ├── cycle_global_mean_squared_error_log\n",
    "    ├── cycle_global_real_mean_squared_error_log\n",
    "    ├── cycle_iterations_time_log\n",
    "    ├── diagonal_global_mean_squared_error_log\n",
    "    ├── diagonal_global_real_mean_squared_error_log\n",
    "    ├── diagonal_iterations_time_log\n",
    "    ├── diam-expander_global_mean_squared_error_log\n",
    "    ├── diam-expander_global_real_mean_squared_error_log\n",
    "    ├── diam-expander_iterations_time_log\n",
    "    ├── root-expander_global_mean_squared_error_log\n",
    "    ├── root-expander_global_real_mean_squared_error_log\n",
    "    ├── root-expander_iterations_time_log\n",
    "    ├── .descriptor.txt\n",
    "    └── .setup.pkl\n",
    "```\n",
    "\n",
    "#### Test descriptor\n",
    "Below is how the descriptor appears, it doesn't matter if some or most of them could be not immediately understandable, simply take a fast look at this code snippet to realize how many parameters the system let us customize.\n",
    "\n",
    "```\n",
    ">>> Test Descriptor File\n",
    "Title: test\n",
    "Date: 2018-04-26 11:28:39.116108\n",
    "Summary: -\n",
    "\n",
    "### BEGIN SETUP ###\n",
    "n = 10\n",
    "seed = 1524734919\n",
    "graphs = {\n",
    "    'clique': np.array([\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]),\n",
    "    'cycle': np.array([\n",
    "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
    "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
    "    'diagonal': np.array([\n",
    "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
    "    'diam-expander': np.array([\n",
    "        [1., 1., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
    "        [1., 1., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
    "        [0., 1., 1., 1., 0., 0., 0., 1., 0., 0.],\n",
    "        [0., 0., 1., 1., 1., 0., 0., 0., 1., 0.],\n",
    "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
    "        [1., 0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
    "        [0., 1., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 1., 1., 1., 0.],\n",
    "        [0., 0., 0., 1., 0., 0., 0., 1., 1., 1.],\n",
    "        [1., 0., 0., 0., 1., 0., 0., 0., 1., 1.]]),\n",
    "    'root-expander': np.array([\n",
    "        [1., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 1., 1., 0., 1., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 1., 1., 0., 1., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 1., 1., 0., 1., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 1., 0., 1., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 1., 1., 0., 1., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 1., 1., 0., 1.],\n",
    "        [1., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
    "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
    "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 1.]]),\n",
    "    'star': np.array([\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [1., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "        [1., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "        [1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]\n",
    "    )}\n",
    "\n",
    "# TRAINING SET SETUP\n",
    "n_samples = 100000\n",
    "n_features = 100\n",
    "yhat = <class 'src.mltoolbox.LinearYHatFunction'>\n",
    "domain_radius = 5\n",
    "domain_center = 0\n",
    "error_mean = 0\n",
    "error_std_dev = 0\n",
    "\n",
    "# CLUSTER SETUP\n",
    "sample_function = <function LinearYHatFunction.f at 0x7f13fa4d00d0>\n",
    "max_iter = None\n",
    "max_time = 100000\n",
    "method = stochastic\n",
    "batch_size = 20\n",
    "activation_func = None\n",
    "loss = <class 'src.mltoolbox.SquaredLossFunction'>\n",
    "penalty = l2\n",
    "epsilon = 0.01\n",
    "alpha = 1e-06\n",
    "learning_rate = constant\n",
    "metrics = all\n",
    "alt_metrics = False\n",
    "shuffle = True\n",
    "verbose = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
