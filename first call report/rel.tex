\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{enumitem}
\usepackage{url}
\usepackage[maxbibnames=99,backend=biber]{biblatex}
\usepackage{listings}

\bibliography{rel}

%amsthm setup
\newtheoremstyle{newplanestyle}%                % Name
{10pt}%                                     % Space above
{0pt}%                                     % Space below
{\itshape}%                             % Body font
{}%                                     % Indent amount
{\bfseries}%                            % Theorem head font
{.}%                                    % Punctuation after theorem head
{ }%                                     % Space after theorem head, ' ', or \newline
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}} % Theorem head spec (can be left empty, meaning `normal')

\newtheoremstyle{newdefinitionstyle}%                % Name
{10pt}%                                     % Space above
{0pt}%                                     % Space below
{}%                             		  % Body font
{}%                                     % Indent amount
{\bfseries}%                            % Theorem head font
{.}%                                    % Punctuation after theorem head
{ }%                                    % Space after theorem head, ' ', or \newline
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}} % Theorem head spec (can be left empty, meaning `normal')

\newtheoremstyle{newprovestyle}%                 % Name
{10pt}%                                 % Space above
{0pt}%                                  % Space below
{}%                             		  % Body font
{}%                                     % Indent amount
{\itshape}%                            % Theorem head font
{.}%                                    % Punctuation after theorem head
{ }%                                    % Space after theorem head, ' ', or \newline
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{newplanestyle}
\newtheorem{newtheo}{Theorem}[section]
\newtheorem{newprop}[newtheo]{Proposition}
\newtheorem{newlem}[newtheo]{Lem}
\newtheorem{newcor}[newtheo]{Corollary}

\theoremstyle{newdefinitionstyle}
\newtheorem{newdef}[newtheo]{Definition}
\newtheorem{newex}[newtheo]{Example}

\theoremstyle{newprovestyle}
\newtheorem*{newprove}{Prove}

\addto{\captionsenglish}{\renewcommand{\abstractname}{Internship brief description}}

%opening
\title{%
  Asynchronous Approximate Distributed Computation for Machine Learning  \\
  First call report}
\author{Gianmarco Calbi}
\date{February 15, 2018}

\begin{document}
\maketitle

\begin{abstract}

\end{abstract}

\clearpage

\section*{Context}
\subsection*{Machine learning training}
The most exploited method to train neural network is the Gradient Descent.

\subsection*{Gradient descent (GD)}
Iterative method to achieve a local optimum of a continuously differentiable function (\textit{convex/concave optimization always achieves an optimum that is \textbf{global}}).

\begin{newdef}[Empirical risk]
The \textit{empirical risk} is a function measuring the training set performance, is the one we'd like to minimize.
\[
E_n(\vec{w})=\frac{1}{n}\sum_{i=1}^{n}loss(f_{\vec{w}}(x_i),y_i)
\]
\end{newdef}
where:
\begin{itemize}
\item $n$ is the size of the training set;
\item the training set is defined as $\mathcal{X} = \{(\vec{x}_i, y_i) : i \in \mathbb{N}\}$ where $\vec{x}_i$ is an input for the neural network and $y_i$ is the correct and desired output for such input;
\item $\vec{w}$ is the weight vector;
\item $loss(y_i, f_{\vec{w}}(x_i))$ is the \textit{loss} function which measures the cost of predicting $f_{\vec{w}}(x_i)$ (usually referred as $\hat{y_i}$) when the correct output should be $y_i$; obviously, $loss(y_i, \hat{y_i})=0 \Leftrightarrow \hat{y_i} = y_i$.
\end{itemize}

Several loss functions have been defined for different types of learning tasks, at the moment we are considering
\[
loss(y_i, \hat{y_i}) = \frac{1}{2}(y_i - \hat{y_i})^2
\]
so that $E_n$ is nothing but the \textit{mean squared error} of the trained function $f_{\vec{w}}(\vec{x})$ prediction over the training set.

GD is the most suitable method to achieve a $\vec{w}$ such that $f_{\vec{w}}$ could be retained a ``good'' approximation of the target function $y$. The \textit{goodness} of the approximation is directly related to $E_n$, usually $E_n$ getting less than or equal to a user-defined small real number $\epsilon$ states that $f_{\vec{w}}$ is good enough predictor.

GD method consists in a stepwise update of $w$ starting from an initial $w_0$ (either fixed or randomly picked):
\begin{equation*}
\vec{w}^{(t+1)} = \vec{w}^{(t)}-\eta \nabla_{\vec{w}} E_n(\vec{w}^{(t)})
= \vec{w}^{(t)} - \frac{\eta}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)(-\hat{y}_i)'
\end{equation*}
where $\eta$ is the \textit{learning rate} (higher learning rate values leads to fast convergence but less accurate solutions while smaller ones do the opposite, it is always about finding a trade-off).

The computation is stopped upon either reaching good solution (as explained right before), or reaching a user-defined maximum number of iterations, or even stating divergence (this case may be avoided).

\subsection*{Stochastic gradient descent (SGD)}
When the size of the training set is such the stepwise update of $w$ requires an unaffordable computation effort, than one would rather rely on the \textit{stochastic gradient descent (SGD)}, that is a simplification of the classic method which outperforms GD w.r.t. single step speed and ensures (almost always under certain conditions) convergence, as well.

In SGD, the empirical risk $E_n$ computed on the whole training set $\mathcal{X}$, is replaced with $E(\vec{w})=loss(f_{\vec{w}}(\vec{x}_p),y_p)$ where $(\vec{x}_p, y_p)$ is a sample randomly picked from $\mathcal{X}$. Hence we aim to minimize the error given only by a single sample rather than the error on the whole training set' samples. As stated before, under certain condition on the training set, approximate $E_n$ with $E$ will lead to converge as well.

The stepwise update of $w$ finally becomes
\[
\vec{w}^{(t+1)}=\vec{w}^{(t)}-\eta \nabla_{\vec{w}} E(\vec{w}^{(t)})=\vec{w}^{(t)}-\eta(y_p - \hat{y}_p)(-\hat{y}_p)'.
\]

\subsection*{Training in a distributed environment}
Let the distributed system be composed of $k$ computational nodes, then the gradient descent is performed following the steps below:
\begin{enumerate}
\item the training set is split into $k$ disjoint subsets $\mathcal{X}_k$;
\item $\mathcal{X}_u$ is assigned to computational node $u$;
\item each node $u$, which owns a local weight vector $\vec{w}_u$, performs a single step update of it obtaining $\vec{w}^{(t)}_u$ ($w$ at $t$-th iteration);
\item then partial solutions from all nodes are averaged to a single $w^{(t)}$;
\item each node updates his local model with $w^{(t)}$;
\item repeat from 3 until a stop condition is met.
\end{enumerate}
In such kind of systems the weight update can be done either with GD or with SGD according to the size of the problem. Usually, the fact of working within a distributed system implicitly suggests that one is dealing with big datasets, therefore SGD is basically mandatory.


\subsection*{Bulk Synchronous Processing}


\section*{Model overview}
We would like to emulate the behavior of a distributed system (that is a cluster of  machines running in parallel) within a single process (single-thread too, so far). Our intention consists, given a number $N$ of computational units (nodes), in finding a trade-off between $N$ and the number of connections established among each machine.


\begin{lstlisting}
print(`Hello!')
\end{lstlisting}


\section*{Model setup}


\section*{Current results/outputs}




\cleardoublepage

\printbibliography %[heading=none]
\nocite{wiki:gip}
\nocite{wiki:msp}

\end{document}
