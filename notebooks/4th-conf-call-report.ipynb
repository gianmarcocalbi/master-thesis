{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4th Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-18 23:19:11.823302\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(str(datetime.datetime.today()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually it's been one month since the last report, therefore here I present a list (not too much detailed) of new ...\n",
    "\n",
    "*specificare che le cose dettagliate sono in un altro report privato*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- time distributions considered\n",
    "    - exponential\n",
    "    - uniform\n",
    "    - Pareto type 2\n",
    "- bound in general\n",
    "    - memoryless lower bound (quick reminder)\n",
    "    - residual lifetime lower bound (full explanation)\n",
    "    - upper bound (full explanation)\n",
    "    - Don's velocity approximation bound (only for exponential)\n",
    "- time distributions considered\n",
    "    - exponential\n",
    "        - all bounds for exponential distribution\n",
    "    - uniform\n",
    "        - all bounds for uniform distribution\n",
    "    - Pareto type 2\n",
    "        - all bounds for pareto type 2\n",
    "- training set points' domain changing\n",
    "    - comparison with different w's components domains\n",
    "- fix applied to code causes error to change a lot\n",
    "    - why?\n",
    "    - new tests\n",
    "    - tests' comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "We've focused on studying iterations' speed.\n",
    "\n",
    "The time taken by node $u$ is a random variable $X_u$, $X_1,...,X_n$ are I.I.D.\n",
    "\n",
    "The single iteration's velocity of $u$ is $1$ over the total time spent to move forward by one iteration.\n",
    "\n",
    "In previous tests the only random distribution taken into account was the Exponential with parameter $\n",
    "\\lambda=1$. Now we've introduced also **Uniform** and **Type II Pareto distributions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time distributions\n",
    "With parameters chosen to achieve $\\mathbb{E}[X] = 1$\n",
    "\n",
    "## Exponential\n",
    "$X \\sim Exp(\\lambda=1)$. Exponential is \n",
    "\n",
    "## Uniform\n",
    "$X \\sim unif(0, 2)$.\n",
    "\n",
    "\n",
    "## Type II Pareto (Lomax)\n",
    "Heavy tailed distribution. $X \\sim Lomax(\\lambda = 3, \\alpha = 2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounds\n",
    "\n",
    "## Memoryless lower bound\n",
    "\n",
    "Let $Y=max(X_1,...,X_k)$ be the slowest dependency node to complete the task,\n",
    "\n",
    "$$Pr(Y \\leq y) = \\prod_{i=1}^{k} Pr(X_i \\leq y) = Pr(X \\leq y)^k = F_X(y)^k.$$\n",
    "\n",
    "The expected value of $Y$ is\n",
    "\n",
    "$$\\mathbb{E}[Y] = \\int_{0}^{\\infty} (1-Pr(Y\\leq y))\\ dy = \\int_{0}^{\\infty} (1-F_X(y)^k)\\ dy.$$\n",
    "\n",
    "The total time $v$ has to wait is $\\mathbb{E}[Y]+\\mathbb{E}[X]$, so the single iteration velocity can be expressed as\n",
    "\n",
    "$$V = \\frac{1}{\\mathbb{E}[Y]+\\mathbb{E}[X]}.$$\n",
    "\n",
    "Since we've considered the slowest node one can conclude that this is a lower bound for the single iteration velocity, but, actually, it is not: we suppose that by the time node $v$ finishes computation for iteration $t$ then its dependencies, in turn, have already received the required informations from their dependencies, so they can start calculations for iteration $t$. This assumption is not always true.\n",
    "\n",
    "There is another assumption this bound does: dependencies of $v$ starts computing only when $v$ finishes iteration $t$. This is not a problem if $X$ follows a distribution which owns the memorylessness property.\n",
    "\n",
    "**Memorylessness property**.\n",
    "*A distribution is memoryless when the \"waiting time\" until a certain event, does not depend on how much time has elapsed already, so a memoryless random variable can be regenerated in any moment. The only continuous and memoryless random distribution is the **exponential one**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual lifetime lower bound\n",
    "This is a lower bound for non-memoryless distributions (Uniform and Pareto Type II).\n",
    "\n",
    "Like in the previous bound, the first supposition persists: when node $u$ finishes computation for iteration $t$, its dependencies already own informations to start computing. But now, this bound takes into account that by the time node $u$ finishes its computation for iteration $t$, some ($0 \\leq some \\leq k$) of its dependencies may have already started computing, hence here we consider *residual lifetime* variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topologies matter\n",
    "From past tests it seems that the topology doesn't, but it actually does, it is not possible, for instance, that a topology with degree 1 performs in the same way as the clique with regards to error over iterations. We think that likely the training set divided among nodes is too homogeneous, so we introduce a per-node error $\\eta_u$ (in addition to the per-example error $\\eta_i$). $\\eta_u$ is randomly sampled from a Gaussian with mean equal to $0$ and arbitrary variance, it is constant within node $u$. It means that for $n$ nodes, there will be as many $\\eta_u$. \n",
    "\n",
    "For each sample $\\mathbf{x}_i$,\n",
    "$$f(\\mathbf{x}_i) = \\langle\\mathbf{x}_i, \\mathbf{w}\\rangle + \\eta_i + \\eta_u.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between two different plots (with and without $\\eta_u$)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation time split in constant and random parts\n",
    "We've introduced a constant part $0 \\leq c \\leq 1$ into single iteration $t$ completion time.\n",
    "\n",
    "$$Time(t) = c*\\mathbb{E}[X] + (1-c)*X(t).$$\n",
    "\n",
    "$c$ can be thought as the weight assigned to the constant part:\n",
    "* $c=0$ implies $Time(t) = X(t)$: exclusively random part;\n",
    "* $c=1$ implies $Time(t) = \\mathbb{E}[X]$: just constant part;\n",
    "* $c=0.5$ implies both parts counts the same;\n",
    "* otherwise: both counts but with different weights.\n",
    "\n",
    "This is helping in testing and tuning topologies' velocity bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New algorithm - Dual averaging method\n",
    "It provides a convergence proof in a distributed environment while the first article we considered didn't use to.\n",
    "\n",
    "This approach is slightly different from the gradient descent method. \n",
    "\n",
    "For the distributed environment each node $u$ owns an additional parameter $z(t),\\ t\\geq0$. The update is then computed as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_u(t+1) = \n",
    "\\begin{cases}\n",
    "    \\frac{1}{N}\\sum_{v \\in \\mathcal{N}(u)}z_v(t) + g_u(t) & \\text{if } t>0 \\\\\n",
    "    0  & \\text{if } t=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{N}(u) = \\{v | (v,u) \\in E\\}$ and $g_u(t)$ is the gradient computed by using $w_u(t)$ by node $u$.\n",
    "\n",
    "Then the update of $w_u$ is made as follows:\n",
    "\n",
    "$$w_u(t+1) = \\operatorname*{argmin}_{\\mathbf{w} \\in W} \\{\\Vert \\mathbf{w} + \\alpha(t)\\ z(t+1)\\Vert_2\\}$$\n",
    "\n",
    "where\n",
    "- $\\Vert\\mathbf{a}\\Vert_2$ is the norm 2 of vector $\\mathbf{a}$ (Euclidean norm);\n",
    "- $W = \\{\\mathbf{w} \\in \\mathbb{R}^p : \\Vert\\mathbf{w}\\Vert_2 \\leq r\\}$ with $r \\in \\mathbb{R}$ a fixed radius;\n",
    "- $\\{\\alpha(t)\\}_{t=0}^\\infty$ is a non increasing sequence of positive stepsizes.\n",
    "\n",
    "This means \n",
    "$$\n",
    "w_u(t+1) =\n",
    "\\begin{cases}\n",
    "    - \\alpha(t)\\ z(t+1) & \\text{if } \\Vert \\alpha(t)\\ z(t+1) \\Vert_2 \\leq r  \\\\\n",
    "    \\frac{- \\alpha(t)\\ z(t+1)}{\\Vert \\alpha(t)\\ z(t+1) \\Vert_2} r & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\frac{- \\alpha(t)\\ z(t+1)}{\\Vert \\alpha(t)\\ z(t+1) \\Vert_2} r$ is the projection of $-\\alpha(t)\\ z(t+1)$ on the $p$-dimensional sphere of radius $r$ and center in $\\mathbb{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-constant learning rate\n",
    "Learning rate is $\\alpha(t) \\propto \\frac{1}{\\sqrt t}$, in particular\n",
    "$$\\alpha(t) = \\frac{\\hat\\alpha}{\\sqrt t}$$\n",
    "where $\\hat\\alpha$ is a constant value (usually it is set equal to constant values used in past simulations, e.g. with order of magnitude $10^{-4}, 10^{-6}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual averaging simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM for classification problem\n",
    "A new minimization function and training set.\n",
    "\n",
    "## Target function\n",
    "The target function is still the same\n",
    "$$f(\\mathbf{w}) := \\frac{1}{n} \\sum_{i=1}^{n} loss(y_i, \\hat y_i).$$\n",
    "\n",
    "The loss function changes to accomodate a classification problem, e.g. the task of slitting samples into two (classification) or more (multi-classification) classes. We're considering the typical two-classification problem.\n",
    "\n",
    "### Hinge loss function\n",
    "The loss function (SVM classifier) is defined as $loss(y_i, \\hat y_i) = max\\{0, 1-y_i \\hat y_i\\}$, where $\\hat y_i = \\langle\\mathbf{x}_i,\\mathbf{w}\\rangle = \\sum_{j=1}^{p}\\mathbf{X}_{ij}\\mathbf{w}_j$.\n",
    "\n",
    "So, the whole target function becomes\n",
    "\n",
    "$$f(\\mathbf{w}) := \\frac{1}{n} \\sum_{i=1}^{n} max\\{0, 1-y_i \\langle\\mathbf{x}_i,\\mathbf{w}\\rangle\\}.$$\n",
    " \n",
    "\n",
    "\n",
    "## Classification training set\n",
    "$\\mathbf{w}$ is random starting weight vector.\n",
    "The training set is $\\mathcal{X} = \\{(\\mathbf{x}_i, y_i)\\}$ such that:\n",
    "* $\\mathbf{x}_i$ is a point belonging to the $p$-dimensional unit sphere (e.g. with radius equal to $1$), so that $\\Vert \\mathbf{x}_i \\Vert_2 = 1$;\n",
    "* $y_i = sign(\\langle \\mathbf{x}_i, \\mathbf{w} \\rangle)$;\n",
    "* an error is introduced by flipping the sign of $5\\%$ of samples, so that $y_s = -sign(\\langle \\mathbf{x}_s, \\mathbf{w} \\rangle)$ if sample $s$ is flipped.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
